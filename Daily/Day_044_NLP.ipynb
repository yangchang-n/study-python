{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_044_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/mbIsldP2Brou7Ymn5Zyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangchangnaihoby/AI_7th/blob/master/Day_044_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename = \"11-0.txt\")\n",
        "\n",
        "f = open('11-0.txt', 'rb')\n",
        "sentences = []\n",
        "for sentence in f :\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.decode('ascii', 'ignore')\n",
        "    if len(sentence) > 0 :\n",
        "        sentences.append(sentence)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "mFW7P7fb4F80"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[ : 5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5qA7WrKzedO",
        "outputId": "8998f53b-4f04-4fe4-815f-64ae804c0969"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
              " 'this ebook is for the use of anyone anywhere in the united states and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
              " 'of the project gutenberg license included with this ebook or online at']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = ' '.join(sentences)\n",
        "print('문자열의 길이 또는 총 문자의 개수 : %d' % len(total_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzERXk-SziYD",
        "outputId": "e560fd49-c13c-4c98-9b94-a814443ed58d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자열의 길이 또는 총 문자의 개수 : 159484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data[ : 200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m47pvKKzl4X",
        "outputId": "3511554a-79d8-4abb-b67c-3411b96be4b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(list(set(total_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbUqXXVgzr6H",
        "outputId": "c9780a5a-e304-469d-a81b-bd66c83cd2bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
        "print('문자 집합 :', char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Zxrv9Qz0RP",
        "outputId": "860b3c21-6652-462b-fbce-f4b8ef761f94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 : {' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char = {}\n",
        "for key, value in char_to_index.items() :\n",
        "    index_to_char[value] = key"
      ],
      "metadata": {
        "id": "L42a_5kzz64W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = 'appl'\n",
        "train_y = 'pple'"
      ],
      "metadata": {
        "id": "9YFBBJQlz-s0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 60\n",
        "\n",
        "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
        "print ('샘플의 수 : {}'.format(n_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzvsuHEa0BFu",
        "outputId": "94a1d53b-c260-44fe-e334-d06915f3f3d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 수 : 2658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "    \n",
        "    X_sample = total_data[i * seq_length : (i + 1) * seq_length]\n",
        "    X_encoded = [char_to_index[c] for c in X_sample]\n",
        "    train_X.append(X_encoded)\n",
        "\n",
        "    y_sample = total_data[i * seq_length + 1 : (i + 1) * seq_length + 1]\n",
        "    y_encoded = [char_to_index[c] for c in y_sample]\n",
        "    train_y.append(y_encoded)"
      ],
      "metadata": {
        "id": "OcblGwF00FRl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X 데이터의 첫번째 샘플 :', train_X[0])\n",
        "print('y 데이터의 첫번째 샘플 :', train_y[0])\n",
        "print('-' * 50)\n",
        "print('X 데이터의 첫번째 샘플 디코딩 :', [index_to_char[i] for i in train_X[0]])\n",
        "print('y 데이터의 첫번째 샘플 디코딩 :', [index_to_char[i] for i in train_y[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bNSGGg10QM8",
        "outputId": "84faf410-313f-4dce-c25a-61e00256f69b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
            "y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
            "--------------------------------------------------\n",
            "X 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
            "y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKs8s77A0X2q",
        "outputId": "c382f9a7-0fc4-47d2-fb24-fe2fa4d704c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKaL9jH-0el9",
        "outputId": "401f8b5d-8534-4e78-dcb9-eacf777f0625"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = to_categorical(train_X)\n",
        "train_y = to_categorical(train_y)\n",
        "\n",
        "print('train_X의 크기(shape) : {}'.format(train_X.shape))\n",
        "print('train_y의 크기(shape) : {}'.format(train_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCAgUDGg0hNf",
        "outputId": "a9643103-8dfa-476a-b0f8-fdaf37e7f808"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X의 크기(shape) : (2658, 60, 56)\n",
            "train_y의 크기(shape) : (2658, 60, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape = (None, train_X.shape[2]), return_sequences = True))\n",
        "model.add(LSTM(hidden_units, return_sequences = True))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation = 'softmax')))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(train_X, train_y, epochs = 80, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf5xCfB00mhr",
        "outputId": "8c594d86-7038-437f-b5de-7c62ad2c2326"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "84/84 - 43s - loss: 3.0717 - accuracy: 0.1854 - 43s/epoch - 513ms/step\n",
            "Epoch 2/80\n",
            "84/84 - 41s - loss: 2.7357 - accuracy: 0.2469 - 41s/epoch - 483ms/step\n",
            "Epoch 3/80\n",
            "84/84 - 39s - loss: 2.3989 - accuracy: 0.3266 - 39s/epoch - 469ms/step\n",
            "Epoch 4/80\n",
            "84/84 - 39s - loss: 2.2491 - accuracy: 0.3615 - 39s/epoch - 470ms/step\n",
            "Epoch 5/80\n",
            "84/84 - 40s - loss: 2.1350 - accuracy: 0.3894 - 40s/epoch - 474ms/step\n",
            "Epoch 6/80\n",
            "84/84 - 39s - loss: 2.0539 - accuracy: 0.4092 - 39s/epoch - 466ms/step\n",
            "Epoch 7/80\n",
            "84/84 - 39s - loss: 1.9828 - accuracy: 0.4270 - 39s/epoch - 466ms/step\n",
            "Epoch 8/80\n",
            "84/84 - 39s - loss: 1.9241 - accuracy: 0.4420 - 39s/epoch - 466ms/step\n",
            "Epoch 9/80\n",
            "84/84 - 40s - loss: 1.8698 - accuracy: 0.4562 - 40s/epoch - 477ms/step\n",
            "Epoch 10/80\n",
            "84/84 - 39s - loss: 1.8233 - accuracy: 0.4694 - 39s/epoch - 466ms/step\n",
            "Epoch 11/80\n",
            "84/84 - 40s - loss: 1.7759 - accuracy: 0.4832 - 40s/epoch - 471ms/step\n",
            "Epoch 12/80\n",
            "84/84 - 39s - loss: 1.7348 - accuracy: 0.4940 - 39s/epoch - 467ms/step\n",
            "Epoch 13/80\n",
            "84/84 - 39s - loss: 1.6965 - accuracy: 0.5054 - 39s/epoch - 467ms/step\n",
            "Epoch 14/80\n",
            "84/84 - 39s - loss: 1.6575 - accuracy: 0.5150 - 39s/epoch - 465ms/step\n",
            "Epoch 15/80\n",
            "84/84 - 39s - loss: 1.6233 - accuracy: 0.5242 - 39s/epoch - 465ms/step\n",
            "Epoch 16/80\n",
            "84/84 - 40s - loss: 1.5895 - accuracy: 0.5334 - 40s/epoch - 480ms/step\n",
            "Epoch 17/80\n",
            "84/84 - 39s - loss: 1.5588 - accuracy: 0.5422 - 39s/epoch - 463ms/step\n",
            "Epoch 18/80\n",
            "84/84 - 39s - loss: 1.5276 - accuracy: 0.5502 - 39s/epoch - 465ms/step\n",
            "Epoch 19/80\n",
            "84/84 - 39s - loss: 1.4960 - accuracy: 0.5595 - 39s/epoch - 464ms/step\n",
            "Epoch 20/80\n",
            "84/84 - 39s - loss: 1.4665 - accuracy: 0.5674 - 39s/epoch - 465ms/step\n",
            "Epoch 21/80\n",
            "84/84 - 39s - loss: 1.4373 - accuracy: 0.5744 - 39s/epoch - 462ms/step\n",
            "Epoch 22/80\n",
            "84/84 - 39s - loss: 1.4114 - accuracy: 0.5827 - 39s/epoch - 463ms/step\n",
            "Epoch 23/80\n",
            "84/84 - 40s - loss: 1.3779 - accuracy: 0.5916 - 40s/epoch - 479ms/step\n",
            "Epoch 24/80\n",
            "84/84 - 39s - loss: 1.3561 - accuracy: 0.5982 - 39s/epoch - 465ms/step\n",
            "Epoch 25/80\n",
            "84/84 - 39s - loss: 1.3257 - accuracy: 0.6070 - 39s/epoch - 467ms/step\n",
            "Epoch 26/80\n",
            "84/84 - 39s - loss: 1.2983 - accuracy: 0.6138 - 39s/epoch - 465ms/step\n",
            "Epoch 27/80\n",
            "84/84 - 39s - loss: 1.2723 - accuracy: 0.6207 - 39s/epoch - 465ms/step\n",
            "Epoch 28/80\n",
            "84/84 - 39s - loss: 1.2455 - accuracy: 0.6294 - 39s/epoch - 460ms/step\n",
            "Epoch 29/80\n",
            "84/84 - 39s - loss: 1.2179 - accuracy: 0.6374 - 39s/epoch - 461ms/step\n",
            "Epoch 30/80\n",
            "84/84 - 40s - loss: 1.1919 - accuracy: 0.6450 - 40s/epoch - 475ms/step\n",
            "Epoch 31/80\n",
            "84/84 - 39s - loss: 1.1627 - accuracy: 0.6534 - 39s/epoch - 463ms/step\n",
            "Epoch 32/80\n",
            "84/84 - 39s - loss: 1.1376 - accuracy: 0.6598 - 39s/epoch - 463ms/step\n",
            "Epoch 33/80\n",
            "84/84 - 39s - loss: 1.1128 - accuracy: 0.6678 - 39s/epoch - 462ms/step\n",
            "Epoch 34/80\n",
            "84/84 - 39s - loss: 1.0826 - accuracy: 0.6765 - 39s/epoch - 462ms/step\n",
            "Epoch 35/80\n",
            "84/84 - 39s - loss: 1.0580 - accuracy: 0.6842 - 39s/epoch - 465ms/step\n",
            "Epoch 36/80\n",
            "84/84 - 39s - loss: 1.0303 - accuracy: 0.6928 - 39s/epoch - 461ms/step\n",
            "Epoch 37/80\n",
            "84/84 - 40s - loss: 0.9995 - accuracy: 0.7026 - 40s/epoch - 475ms/step\n",
            "Epoch 38/80\n",
            "84/84 - 39s - loss: 0.9760 - accuracy: 0.7091 - 39s/epoch - 465ms/step\n",
            "Epoch 39/80\n",
            "84/84 - 39s - loss: 0.9509 - accuracy: 0.7172 - 39s/epoch - 465ms/step\n",
            "Epoch 40/80\n",
            "84/84 - 39s - loss: 0.9200 - accuracy: 0.7266 - 39s/epoch - 466ms/step\n",
            "Epoch 41/80\n",
            "84/84 - 39s - loss: 0.8943 - accuracy: 0.7343 - 39s/epoch - 465ms/step\n",
            "Epoch 42/80\n",
            "84/84 - 39s - loss: 0.8670 - accuracy: 0.7425 - 39s/epoch - 465ms/step\n",
            "Epoch 43/80\n",
            "84/84 - 40s - loss: 0.8429 - accuracy: 0.7506 - 40s/epoch - 476ms/step\n",
            "Epoch 44/80\n",
            "84/84 - 39s - loss: 0.8170 - accuracy: 0.7587 - 39s/epoch - 464ms/step\n",
            "Epoch 45/80\n",
            "84/84 - 39s - loss: 0.7954 - accuracy: 0.7658 - 39s/epoch - 465ms/step\n",
            "Epoch 46/80\n",
            "84/84 - 39s - loss: 0.7644 - accuracy: 0.7752 - 39s/epoch - 463ms/step\n",
            "Epoch 47/80\n",
            "84/84 - 39s - loss: 0.7445 - accuracy: 0.7817 - 39s/epoch - 464ms/step\n",
            "Epoch 48/80\n",
            "84/84 - 39s - loss: 0.7208 - accuracy: 0.7892 - 39s/epoch - 467ms/step\n",
            "Epoch 49/80\n",
            "84/84 - 39s - loss: 0.7004 - accuracy: 0.7959 - 39s/epoch - 469ms/step\n",
            "Epoch 50/80\n",
            "84/84 - 40s - loss: 0.6710 - accuracy: 0.8060 - 40s/epoch - 471ms/step\n",
            "Epoch 51/80\n",
            "84/84 - 40s - loss: 0.6418 - accuracy: 0.8153 - 40s/epoch - 480ms/step\n",
            "Epoch 52/80\n",
            "84/84 - 39s - loss: 0.6230 - accuracy: 0.8208 - 39s/epoch - 463ms/step\n",
            "Epoch 53/80\n",
            "84/84 - 39s - loss: 0.6041 - accuracy: 0.8263 - 39s/epoch - 465ms/step\n",
            "Epoch 54/80\n",
            "84/84 - 39s - loss: 0.5910 - accuracy: 0.8294 - 39s/epoch - 467ms/step\n",
            "Epoch 55/80\n",
            "84/84 - 39s - loss: 0.5555 - accuracy: 0.8434 - 39s/epoch - 464ms/step\n",
            "Epoch 56/80\n",
            "84/84 - 39s - loss: 0.5341 - accuracy: 0.8496 - 39s/epoch - 468ms/step\n",
            "Epoch 57/80\n",
            "84/84 - 40s - loss: 0.5164 - accuracy: 0.8550 - 40s/epoch - 472ms/step\n",
            "Epoch 58/80\n",
            "84/84 - 41s - loss: 0.5064 - accuracy: 0.8567 - 41s/epoch - 484ms/step\n",
            "Epoch 59/80\n",
            "84/84 - 39s - loss: 0.4892 - accuracy: 0.8634 - 39s/epoch - 470ms/step\n",
            "Epoch 60/80\n",
            "84/84 - 39s - loss: 0.4697 - accuracy: 0.8689 - 39s/epoch - 468ms/step\n",
            "Epoch 61/80\n",
            "84/84 - 39s - loss: 0.4471 - accuracy: 0.8778 - 39s/epoch - 469ms/step\n",
            "Epoch 62/80\n",
            "84/84 - 40s - loss: 0.4399 - accuracy: 0.8783 - 40s/epoch - 472ms/step\n",
            "Epoch 63/80\n",
            "84/84 - 40s - loss: 0.4283 - accuracy: 0.8819 - 40s/epoch - 470ms/step\n",
            "Epoch 64/80\n",
            "84/84 - 39s - loss: 0.3931 - accuracy: 0.8952 - 39s/epoch - 470ms/step\n",
            "Epoch 65/80\n",
            "84/84 - 39s - loss: 0.3784 - accuracy: 0.8999 - 39s/epoch - 470ms/step\n",
            "Epoch 66/80\n",
            "84/84 - 41s - loss: 0.3709 - accuracy: 0.9020 - 41s/epoch - 485ms/step\n",
            "Epoch 67/80\n",
            "84/84 - 39s - loss: 0.3536 - accuracy: 0.9070 - 39s/epoch - 470ms/step\n",
            "Epoch 68/80\n",
            "84/84 - 39s - loss: 0.3375 - accuracy: 0.9129 - 39s/epoch - 469ms/step\n",
            "Epoch 69/80\n",
            "84/84 - 39s - loss: 0.3311 - accuracy: 0.9135 - 39s/epoch - 469ms/step\n",
            "Epoch 70/80\n",
            "84/84 - 39s - loss: 0.3381 - accuracy: 0.9091 - 39s/epoch - 470ms/step\n",
            "Epoch 71/80\n",
            "84/84 - 40s - loss: 0.3180 - accuracy: 0.9167 - 40s/epoch - 470ms/step\n",
            "Epoch 72/80\n",
            "84/84 - 40s - loss: 0.2927 - accuracy: 0.9260 - 40s/epoch - 471ms/step\n",
            "Epoch 73/80\n",
            "84/84 - 41s - loss: 0.2828 - accuracy: 0.9290 - 41s/epoch - 483ms/step\n",
            "Epoch 74/80\n",
            "84/84 - 40s - loss: 0.2829 - accuracy: 0.9277 - 40s/epoch - 472ms/step\n",
            "Epoch 75/80\n",
            "84/84 - 39s - loss: 0.2765 - accuracy: 0.9294 - 39s/epoch - 470ms/step\n",
            "Epoch 76/80\n",
            "84/84 - 40s - loss: 0.2482 - accuracy: 0.9394 - 40s/epoch - 477ms/step\n",
            "Epoch 77/80\n",
            "84/84 - 40s - loss: 0.2394 - accuracy: 0.9415 - 40s/epoch - 474ms/step\n",
            "Epoch 78/80\n",
            "84/84 - 40s - loss: 0.2371 - accuracy: 0.9425 - 40s/epoch - 475ms/step\n",
            "Epoch 79/80\n",
            "84/84 - 40s - loss: 0.2396 - accuracy: 0.9398 - 40s/epoch - 477ms/step\n",
            "Epoch 80/80\n",
            "84/84 - 41s - loss: 0.2383 - accuracy: 0.9396 - 41s/epoch - 494ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff245e70450>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, length) :\n",
        "\n",
        "    ix = [np.random.randint(vocab_size)]\n",
        "    y_char = [index_to_char[ix[-1]]]\n",
        "    print(ix[-1], '번 문자', y_char[-1], '로 예측')\n",
        "    X = np.zeros((1, length, vocab_size))\n",
        "\n",
        "    for i in range(length) :\n",
        "        X[0][i][ix[-1]] = 1\n",
        "        print(index_to_char[ix[-1]], end = '')\n",
        "        ix = np.argmax(model.predict(X[ : , : i + 1, : ])[0], 1)\n",
        "        y_char.append(index_to_char[ix[-1]])\n",
        "\n",
        "    return ('').join(y_char)"
      ],
      "metadata": {
        "id": "MWPKDXmn04ZQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = sentence_generation(model, 100)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAwvhx4RDAiY",
        "outputId": "a6196442-0e74-4fb3-bbbc-c55a6ea6759d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42 번 문자 m 로 예측\n",
            "maller, i can creep under the door; so either way ill get into her, she said with his hasty, and shemaller, i can creep under the door; so either way ill get into her, she said with his hasty, and shes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = '''\n",
        "I get on with life as a programmer,\n",
        "I like to contemplate beer.\n",
        "But when I start to daydream,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "Do I love wine more than beer?\n",
        "\n",
        "I like to use words about beer.\n",
        "But when I stop my talking,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "I hate bugs and errors.\n",
        "But I just think back to wine,\n",
        "And I'm happy once again.\n",
        "\n",
        "I like to hang out with programming and deep learning.\n",
        "But when left alone,\n",
        "My mind turns straight to wine.\n",
        "'''"
      ],
      "metadata": {
        "id": "xgM-7TGfDD_l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgKInKxVEj_U",
        "outputId": "9c4a8b6a-534d-4bca-928c-17f496a25bac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(list(set(raw_text)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합 :', char_vocab)\n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da-pvvFEEmLw",
        "outputId": "74ba7a44-c1e0-4f80-fc3d-442d8331ba5d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 : [' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
            "문자 집합의 크기 : 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcmm4MJOErVQ",
        "outputId": "4fb0e839-37c1-42bc-c173-a6cb7dd0158e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length = 11\n",
        "sequences = []\n",
        "for i in range(length, len(raw_text)) :\n",
        "    seq = raw_text[i - length : i]\n",
        "    sequences.append(seq)\n",
        "print('총 훈련 샘플의 수: %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XqBBPrqEvBZ",
        "outputId": "01e760af-1fd2-4aa9-a58f-01ce8bb88cd9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 훈련 샘플의 수: 426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[30 : 45]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hZpOLXCE3Ts",
        "outputId": "9312a4fd-0b8a-4dbb-a385-41582e1f0952"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mmer, I lik',\n",
              " 'mer, I like',\n",
              " 'er, I like ',\n",
              " 'r, I like t',\n",
              " ', I like to',\n",
              " ' I like to ',\n",
              " 'I like to c',\n",
              " ' like to co',\n",
              " 'like to con',\n",
              " 'ike to cont',\n",
              " 'ke to conte',\n",
              " 'e to contem',\n",
              " ' to contemp',\n",
              " 'to contempl',\n",
              " 'o contempla']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for sequence in sequences :\n",
        "    encoded_sequence = [char_to_index[char] for char in sequence]\n",
        "    encoded_sequences.append(encoded_sequence)"
      ],
      "metadata": {
        "id": "To7NHuoNE6tN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences[20 : 35]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj6iZzQsFBej",
        "outputId": "92b4974e-82cf-4a13-ff5c-1c9be78b6d92"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[27, 0, 10, 0, 25, 26, 24, 16, 26, 10, 22],\n",
              " [0, 10, 0, 25, 26, 24, 16, 26, 10, 22, 22],\n",
              " [10, 0, 25, 26, 24, 16, 26, 10, 22, 22, 14],\n",
              " [0, 25, 26, 24, 16, 26, 10, 22, 22, 14, 26],\n",
              " [25, 26, 24, 16, 26, 10, 22, 22, 14, 26, 2],\n",
              " [26, 24, 16, 26, 10, 22, 22, 14, 26, 2, 0],\n",
              " [24, 16, 26, 10, 22, 22, 14, 26, 2, 0, 8],\n",
              " [16, 26, 10, 22, 22, 14, 26, 2, 0, 8, 0],\n",
              " [26, 10, 22, 22, 14, 26, 2, 0, 8, 0, 21],\n",
              " [10, 22, 22, 14, 26, 2, 0, 8, 0, 21, 18],\n",
              " [22, 22, 14, 26, 2, 0, 8, 0, 21, 18, 20],\n",
              " [22, 14, 26, 2, 0, 8, 0, 21, 18, 20, 14],\n",
              " [14, 26, 2, 0, 8, 0, 21, 18, 20, 14, 0],\n",
              " [26, 2, 0, 8, 0, 21, 18, 20, 14, 0, 28],\n",
              " [2, 0, 8, 0, 21, 18, 20, 14, 0, 28, 24]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = np.array(encoded_sequences)\n",
        "\n",
        "X_data = encoded_sequences[ : , : -1]\n",
        "y_data = encoded_sequences[ : , -1]"
      ],
      "metadata": {
        "id": "x5vPSvFEFEiB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[ : 5])\n",
        "print(y_data[ : 5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEhloxU4FKyG",
        "outputId": "a87225fb-e00f-4ac5-cdc4-fbb287ee2581"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8  0 16 14 28  0 24 23  0 31]\n",
            " [ 0 16 14 28  0 24 23  0 31 18]\n",
            " [16 14 28  0 24 23  0 31 18 28]\n",
            " [14 28  0 24 23  0 31 18 28 17]\n",
            " [28  0 24 23  0 31 18 28 17  0]]\n",
            "[18 28 17  0 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_data_one_hot = [to_categorical(encoded, num_classes = vocab_size) for encoded in X_data]\n",
        "X_data_one_hot = np.array(X_data_one_hot)\n",
        "y_data_one_hot = to_categorical(y_data, num_classes = vocab_size)"
      ],
      "metadata": {
        "id": "viovEtnyFNpl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAptrjVVFX3Z",
        "outputId": "a488c982-7a38-4092-8f5f-b123d78311f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426, 10, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "hidden_units = 64\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(LSTM(hidden_units, input_shape = (X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
        "model_2.add(Dense(vocab_size, activation = 'softmax'))\n",
        "\n",
        "model_2.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model_2.fit(X_data_one_hot, y_data_one_hot, epochs = 100, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAzKNAqJFZze",
        "outputId": "8f197302-2e6f-41cb-fcd9-243883eab6d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/14 - 2s - loss: 3.4629 - accuracy: 0.1244 - 2s/epoch - 167ms/step\n",
            "Epoch 2/100\n",
            "14/14 - 0s - loss: 3.3354 - accuracy: 0.1972 - 97ms/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "14/14 - 0s - loss: 3.0737 - accuracy: 0.1972 - 92ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "14/14 - 0s - loss: 2.9853 - accuracy: 0.1972 - 106ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "14/14 - 0s - loss: 2.9508 - accuracy: 0.1972 - 96ms/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "14/14 - 0s - loss: 2.9345 - accuracy: 0.1972 - 97ms/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "14/14 - 0s - loss: 2.9118 - accuracy: 0.1972 - 101ms/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "14/14 - 0s - loss: 2.8898 - accuracy: 0.1972 - 95ms/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "14/14 - 0s - loss: 2.8782 - accuracy: 0.1972 - 108ms/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "14/14 - 0s - loss: 2.8423 - accuracy: 0.1995 - 101ms/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "14/14 - 0s - loss: 2.8169 - accuracy: 0.1972 - 98ms/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "14/14 - 0s - loss: 2.7714 - accuracy: 0.2136 - 92ms/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "14/14 - 0s - loss: 2.7297 - accuracy: 0.2089 - 95ms/epoch - 7ms/step\n",
            "Epoch 14/100\n",
            "14/14 - 0s - loss: 2.6830 - accuracy: 0.2441 - 101ms/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "14/14 - 0s - loss: 2.6524 - accuracy: 0.2441 - 93ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "14/14 - 0s - loss: 2.6045 - accuracy: 0.2394 - 106ms/epoch - 8ms/step\n",
            "Epoch 17/100\n",
            "14/14 - 0s - loss: 2.5446 - accuracy: 0.2864 - 104ms/epoch - 7ms/step\n",
            "Epoch 18/100\n",
            "14/14 - 0s - loss: 2.5007 - accuracy: 0.2934 - 121ms/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "14/14 - 0s - loss: 2.4651 - accuracy: 0.3005 - 110ms/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "14/14 - 0s - loss: 2.4038 - accuracy: 0.3099 - 97ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "14/14 - 0s - loss: 2.3709 - accuracy: 0.3192 - 98ms/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "14/14 - 0s - loss: 2.3390 - accuracy: 0.3146 - 96ms/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "14/14 - 0s - loss: 2.2755 - accuracy: 0.3451 - 92ms/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "14/14 - 0s - loss: 2.2357 - accuracy: 0.3779 - 101ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "14/14 - 0s - loss: 2.1828 - accuracy: 0.3873 - 93ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "14/14 - 0s - loss: 2.1536 - accuracy: 0.3897 - 113ms/epoch - 8ms/step\n",
            "Epoch 27/100\n",
            "14/14 - 0s - loss: 2.1382 - accuracy: 0.3873 - 96ms/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "14/14 - 0s - loss: 2.0716 - accuracy: 0.4131 - 100ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "14/14 - 0s - loss: 2.0457 - accuracy: 0.4178 - 109ms/epoch - 8ms/step\n",
            "Epoch 30/100\n",
            "14/14 - 0s - loss: 2.0126 - accuracy: 0.4343 - 100ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "14/14 - 0s - loss: 1.9500 - accuracy: 0.4531 - 100ms/epoch - 7ms/step\n",
            "Epoch 32/100\n",
            "14/14 - 0s - loss: 1.9135 - accuracy: 0.4695 - 100ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "14/14 - 0s - loss: 1.8819 - accuracy: 0.4624 - 95ms/epoch - 7ms/step\n",
            "Epoch 34/100\n",
            "14/14 - 0s - loss: 1.8442 - accuracy: 0.4718 - 101ms/epoch - 7ms/step\n",
            "Epoch 35/100\n",
            "14/14 - 0s - loss: 1.7961 - accuracy: 0.5188 - 101ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "14/14 - 0s - loss: 1.7643 - accuracy: 0.5376 - 100ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "14/14 - 0s - loss: 1.7151 - accuracy: 0.5376 - 99ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "14/14 - 0s - loss: 1.7013 - accuracy: 0.5164 - 109ms/epoch - 8ms/step\n",
            "Epoch 39/100\n",
            "14/14 - 0s - loss: 1.6791 - accuracy: 0.5516 - 101ms/epoch - 7ms/step\n",
            "Epoch 40/100\n",
            "14/14 - 0s - loss: 1.6293 - accuracy: 0.5634 - 94ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "14/14 - 0s - loss: 1.5879 - accuracy: 0.6103 - 100ms/epoch - 7ms/step\n",
            "Epoch 42/100\n",
            "14/14 - 0s - loss: 1.5465 - accuracy: 0.6033 - 94ms/epoch - 7ms/step\n",
            "Epoch 43/100\n",
            "14/14 - 0s - loss: 1.5424 - accuracy: 0.6221 - 92ms/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "14/14 - 0s - loss: 1.5073 - accuracy: 0.6197 - 111ms/epoch - 8ms/step\n",
            "Epoch 45/100\n",
            "14/14 - 0s - loss: 1.4697 - accuracy: 0.6385 - 91ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "14/14 - 0s - loss: 1.4210 - accuracy: 0.6549 - 97ms/epoch - 7ms/step\n",
            "Epoch 47/100\n",
            "14/14 - 0s - loss: 1.3676 - accuracy: 0.6784 - 99ms/epoch - 7ms/step\n",
            "Epoch 48/100\n",
            "14/14 - 0s - loss: 1.3497 - accuracy: 0.6737 - 107ms/epoch - 8ms/step\n",
            "Epoch 49/100\n",
            "14/14 - 0s - loss: 1.3119 - accuracy: 0.6925 - 92ms/epoch - 7ms/step\n",
            "Epoch 50/100\n",
            "14/14 - 0s - loss: 1.2862 - accuracy: 0.6925 - 101ms/epoch - 7ms/step\n",
            "Epoch 51/100\n",
            "14/14 - 0s - loss: 1.2555 - accuracy: 0.7019 - 99ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "14/14 - 0s - loss: 1.2235 - accuracy: 0.7066 - 101ms/epoch - 7ms/step\n",
            "Epoch 53/100\n",
            "14/14 - 0s - loss: 1.1804 - accuracy: 0.7136 - 97ms/epoch - 7ms/step\n",
            "Epoch 54/100\n",
            "14/14 - 0s - loss: 1.1717 - accuracy: 0.7277 - 91ms/epoch - 7ms/step\n",
            "Epoch 55/100\n",
            "14/14 - 0s - loss: 1.1308 - accuracy: 0.7254 - 100ms/epoch - 7ms/step\n",
            "Epoch 56/100\n",
            "14/14 - 0s - loss: 1.1100 - accuracy: 0.7441 - 92ms/epoch - 7ms/step\n",
            "Epoch 57/100\n",
            "14/14 - 0s - loss: 1.0844 - accuracy: 0.7465 - 106ms/epoch - 8ms/step\n",
            "Epoch 58/100\n",
            "14/14 - 0s - loss: 1.0593 - accuracy: 0.7347 - 105ms/epoch - 7ms/step\n",
            "Epoch 59/100\n",
            "14/14 - 0s - loss: 1.0218 - accuracy: 0.7723 - 92ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "14/14 - 0s - loss: 0.9995 - accuracy: 0.7582 - 95ms/epoch - 7ms/step\n",
            "Epoch 61/100\n",
            "14/14 - 0s - loss: 0.9551 - accuracy: 0.7840 - 97ms/epoch - 7ms/step\n",
            "Epoch 62/100\n",
            "14/14 - 0s - loss: 0.9431 - accuracy: 0.7934 - 93ms/epoch - 7ms/step\n",
            "Epoch 63/100\n",
            "14/14 - 0s - loss: 0.9086 - accuracy: 0.8052 - 99ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "14/14 - 0s - loss: 0.8893 - accuracy: 0.8052 - 96ms/epoch - 7ms/step\n",
            "Epoch 65/100\n",
            "14/14 - 0s - loss: 0.8609 - accuracy: 0.8052 - 93ms/epoch - 7ms/step\n",
            "Epoch 66/100\n",
            "14/14 - 0s - loss: 0.8473 - accuracy: 0.8216 - 97ms/epoch - 7ms/step\n",
            "Epoch 67/100\n",
            "14/14 - 0s - loss: 0.8112 - accuracy: 0.8216 - 98ms/epoch - 7ms/step\n",
            "Epoch 68/100\n",
            "14/14 - 0s - loss: 0.8045 - accuracy: 0.8239 - 107ms/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "14/14 - 0s - loss: 0.7783 - accuracy: 0.8357 - 97ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "14/14 - 0s - loss: 0.7557 - accuracy: 0.8216 - 92ms/epoch - 7ms/step\n",
            "Epoch 71/100\n",
            "14/14 - 0s - loss: 0.7271 - accuracy: 0.8498 - 92ms/epoch - 7ms/step\n",
            "Epoch 72/100\n",
            "14/14 - 0s - loss: 0.7061 - accuracy: 0.8568 - 98ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "14/14 - 0s - loss: 0.6869 - accuracy: 0.8498 - 94ms/epoch - 7ms/step\n",
            "Epoch 74/100\n",
            "14/14 - 0s - loss: 0.6580 - accuracy: 0.8732 - 93ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "14/14 - 0s - loss: 0.6415 - accuracy: 0.8756 - 96ms/epoch - 7ms/step\n",
            "Epoch 76/100\n",
            "14/14 - 0s - loss: 0.6331 - accuracy: 0.8850 - 93ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "14/14 - 0s - loss: 0.6047 - accuracy: 0.8850 - 102ms/epoch - 7ms/step\n",
            "Epoch 78/100\n",
            "14/14 - 0s - loss: 0.6000 - accuracy: 0.8873 - 103ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "14/14 - 0s - loss: 0.5877 - accuracy: 0.8967 - 91ms/epoch - 7ms/step\n",
            "Epoch 80/100\n",
            "14/14 - 0s - loss: 0.5523 - accuracy: 0.9014 - 97ms/epoch - 7ms/step\n",
            "Epoch 81/100\n",
            "14/14 - 0s - loss: 0.5364 - accuracy: 0.9108 - 97ms/epoch - 7ms/step\n",
            "Epoch 82/100\n",
            "14/14 - 0s - loss: 0.5322 - accuracy: 0.9131 - 120ms/epoch - 9ms/step\n",
            "Epoch 83/100\n",
            "14/14 - 0s - loss: 0.5163 - accuracy: 0.9178 - 98ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "14/14 - 0s - loss: 0.5002 - accuracy: 0.9178 - 103ms/epoch - 7ms/step\n",
            "Epoch 85/100\n",
            "14/14 - 0s - loss: 0.4819 - accuracy: 0.9249 - 101ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "14/14 - 0s - loss: 0.4773 - accuracy: 0.9413 - 102ms/epoch - 7ms/step\n",
            "Epoch 87/100\n",
            "14/14 - 0s - loss: 0.4578 - accuracy: 0.9366 - 94ms/epoch - 7ms/step\n",
            "Epoch 88/100\n",
            "14/14 - 0s - loss: 0.4396 - accuracy: 0.9437 - 108ms/epoch - 8ms/step\n",
            "Epoch 89/100\n",
            "14/14 - 0s - loss: 0.4281 - accuracy: 0.9460 - 104ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "14/14 - 0s - loss: 0.4160 - accuracy: 0.9437 - 92ms/epoch - 7ms/step\n",
            "Epoch 91/100\n",
            "14/14 - 0s - loss: 0.4105 - accuracy: 0.9531 - 96ms/epoch - 7ms/step\n",
            "Epoch 92/100\n",
            "14/14 - 0s - loss: 0.4146 - accuracy: 0.9413 - 90ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "14/14 - 0s - loss: 0.3858 - accuracy: 0.9577 - 104ms/epoch - 7ms/step\n",
            "Epoch 94/100\n",
            "14/14 - 0s - loss: 0.3885 - accuracy: 0.9507 - 95ms/epoch - 7ms/step\n",
            "Epoch 95/100\n",
            "14/14 - 0s - loss: 0.3703 - accuracy: 0.9577 - 99ms/epoch - 7ms/step\n",
            "Epoch 96/100\n",
            "14/14 - 0s - loss: 0.3591 - accuracy: 0.9577 - 102ms/epoch - 7ms/step\n",
            "Epoch 97/100\n",
            "14/14 - 0s - loss: 0.3450 - accuracy: 0.9671 - 97ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "14/14 - 0s - loss: 0.3376 - accuracy: 0.9648 - 100ms/epoch - 7ms/step\n",
            "Epoch 99/100\n",
            "14/14 - 0s - loss: 0.3291 - accuracy: 0.9671 - 93ms/epoch - 7ms/step\n",
            "Epoch 100/100\n",
            "14/14 - 0s - loss: 0.3214 - accuracy: 0.9695 - 92ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2431e25d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, char_to_index, seq_length, seed_text, n) :\n",
        "\n",
        "    init_text = seed_text\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n) :\n",
        "\n",
        "        encoded = [char_to_index[char] for char in seed_text]\n",
        "        encoded = pad_sequences([encoded], maxlen = seq_length, padding = 'pre')\n",
        "        encoded = to_categorical(encoded, num_classes = len(char_to_index))\n",
        "\n",
        "        result = model.predict(encoded, verbose = 0)\n",
        "        result = np.argmax(result, axis = 1)\n",
        "\n",
        "        for char, index in char_to_index.items() :\n",
        "            if index == result :\n",
        "                break\n",
        "\n",
        "        seed_text = seed_text + char\n",
        "        sentence = sentence + char\n",
        "\n",
        "    sentence = init_text + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "lKcXwRojFvb1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model_2, char_to_index, 10, 'I get on w', 80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZA06PHFGdzD",
        "outputId": "d44c5897-914d-4df0-def4-a8cf8c71e2a7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to contemplate beer. But when I start to daydre\n"
          ]
        }
      ]
    }
  ]
}